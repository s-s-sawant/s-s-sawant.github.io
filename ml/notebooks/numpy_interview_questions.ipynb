{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "numpy-intro",
   "metadata": {},
   "source": [
    "# NumPy for Data Science Interviews: 10 Essential Exercises\n",
    "\n",
    "## ðŸ”¢ Master Numerical Computing for ML/DS Roles\n",
    "\n",
    "Welcome to this comprehensive NumPy practice notebook designed specifically for data science and machine learning interviews! These exercises cover the most commonly asked NumPy concepts in technical interviews, from basic array operations to advanced mathematical computations used in real ML workflows.\n",
    "\n",
    "### ðŸ“š What You'll Master\n",
    "- Array creation, indexing, and broadcasting\n",
    "- Statistical operations and data analysis\n",
    "- Linear algebra for machine learning\n",
    "- Data preprocessing and feature engineering\n",
    "- Performance optimization techniques\n",
    "- Real-world ML data manipulation scenarios\n",
    "\n",
    "### ðŸŽ¯ Interview Focus Areas\n",
    "- **Entry Level**: Basic operations, indexing, simple statistics\n",
    "- **Mid Level**: Broadcasting, linear algebra, optimization, feature engineering\n",
    "- **Advanced**: Custom functions, performance considerations, ML implementations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "### 1. Array Creation and Basic Operations (Entry Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Basic NumPy array creation and manipulation - fundamental skill for any DS role.\n",
    "\n",
    "**Key concepts**: np.array(), np.zeros(), np.ones(), np.arange(), np.linspace(), array properties\n",
    "\n",
    "**Common Questions**: \"How do you create arrays of different types?\" \"What's the difference between np.arange() and np.linspace()?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create different types of arrays commonly used in data science\n",
    "\n",
    "# 1. Create a 1D array of integers from 0 to 9\n",
    "arr_1d = # Your code here\n",
    "\n",
    "# 2. Create a 3x4 matrix of zeros (common for initializing weights)\n",
    "zeros_matrix = # Your code here\n",
    "\n",
    "# 3. Create a 2x3 matrix of ones with float32 dtype (memory efficient)\n",
    "ones_matrix = # Your code here\n",
    "\n",
    "# 4. Create an array of 50 evenly spaced values between 0 and 1 (for plotting/analysis)\n",
    "linspace_arr = # Your code here\n",
    "\n",
    "# 5. Create a 5x5 identity matrix (used in linear algebra)\n",
    "identity_matrix = # Your code here\n",
    "\n",
    "# 6. Create a random array with shape (100,) from normal distribution (simulating data)\n",
    "np.random.seed(42)\n",
    "random_data = # Your code here\n",
    "\n",
    "print(f\"1D array: {arr_1d}\")\n",
    "print(f\"Zeros matrix shape: {zeros_matrix.shape}, dtype: {zeros_matrix.dtype}\")\n",
    "print(f\"Ones matrix: \\n{ones_matrix}\")\n",
    "print(f\"Linspace first 5 values: {linspace_arr[:5]}\")\n",
    "print(f\"Identity matrix: \\n{identity_matrix}\")\n",
    "print(f\"Random data stats - mean: {random_data.mean():.3f}, std: {random_data.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify array creation\n",
    "print(f\"1D array correct: {len(arr_1d) == 10 and arr_1d[0] == 0 and arr_1d[-1] == 9}\")\n",
    "print(f\"Zeros matrix correct: {zeros_matrix.shape == (3, 4) and np.all(zeros_matrix == 0)}\")\n",
    "print(f\"Ones matrix correct: {ones_matrix.shape == (2, 3) and ones_matrix.dtype == np.float32}\")\n",
    "print(f\"Identity matrix correct: {np.allclose(identity_matrix, np.eye(5))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "### 2. Array Indexing and Slicing for Data Selection (Entry Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Data selection and filtering - critical for data preprocessing and feature selection.\n",
    "\n",
    "**Key concepts**: Boolean indexing, fancy indexing, slicing, conditional selection\n",
    "\n",
    "**Common Questions**: \"How do you select data based on conditions?\" \"Extract specific rows/columns from a dataset?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset (similar to what you'd get from a CSV)\n",
    "np.random.seed(123)\n",
    "data = np.random.randn(100, 5)  # 100 samples, 5 features\n",
    "labels = np.random.choice([0, 1], 100)  # Binary classification labels\n",
    "\n",
    "# Indexing and slicing exercises\n",
    "\n",
    "# 1. Select first 10 rows and first 3 columns\n",
    "subset = # Your code here\n",
    "\n",
    "# 2. Select all rows where the first feature > 0 (positive values)\n",
    "positive_first_feature = # Your code here\n",
    "\n",
    "# 3. Select samples where label == 1 (positive class)\n",
    "positive_samples = # Your code here\n",
    "\n",
    "# 4. Select rows where any feature value > 2 (outlier detection)\n",
    "outlier_rows = # Your code here\n",
    "\n",
    "# 5. Select specific rows by index (e.g., indices [5, 10, 15, 20])\n",
    "specific_rows = # Your code here\n",
    "\n",
    "# 6. Select last column (often the target variable)\n",
    "last_column = # Your code here\n",
    "\n",
    "# 7. Select every other row (data sampling)\n",
    "sampled_data = # Your code here\n",
    "\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "print(f\"Subset shape: {subset.shape}\")\n",
    "print(f\"Positive first feature samples: {len(positive_first_feature)}\")\n",
    "print(f\"Positive class samples: {len(positive_samples)}\")\n",
    "print(f\"Outlier rows: {len(outlier_rows)}\")\n",
    "print(f\"Specific rows shape: {specific_rows.shape}\")\n",
    "print(f\"Sampled data shape: {sampled_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify indexing operations\n",
    "print(f\"Subset correct: {subset.shape == (10, 3)}\")\n",
    "print(f\"Positive filtering works: {np.all(positive_first_feature[:, 0] > 0)}\")\n",
    "print(f\"Label filtering works: {len(positive_samples) == np.sum(labels == 1)}\")\n",
    "print(f\"Specific rows correct: {specific_rows.shape == (4, 5)}\")\n",
    "print(f\"Sampling correct: {sampled_data.shape == (50, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3",
   "metadata": {},
   "source": [
    "### 3. Statistical Operations for Data Analysis (Entry-Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Descriptive statistics and data summarization - essential for EDA and feature analysis.\n",
    "\n",
    "**Key concepts**: np.mean(), np.std(), np.percentile(), axis parameter, aggregation functions\n",
    "\n",
    "**Common Questions**: \"Calculate statistics along different axes?\" \"How to handle missing data in calculations?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with some missing values (NaN)\n",
    "np.random.seed(456)\n",
    "features = np.random.randn(200, 8)  # 200 samples, 8 features\n",
    "\n",
    "# Introduce some missing values\n",
    "missing_mask = np.random.random((200, 8)) < 0.1  # 10% missing values\n",
    "features[missing_mask] = np.nan\n",
    "\n",
    "# Statistical operations\n",
    "\n",
    "# 1. Calculate mean for each feature (column-wise)\n",
    "feature_means = # Your code here (handle NaN)\n",
    "\n",
    "# 2. Calculate standard deviation for each sample (row-wise)\n",
    "sample_stds = # Your code here (handle NaN)\n",
    "\n",
    "# 3. Find 25th, 50th, and 75th percentiles for each feature\n",
    "percentiles = # Your code here (handle NaN)\n",
    "\n",
    "# 4. Calculate correlation matrix between features\n",
    "# First, create data without NaN for correlation\n",
    "clean_features = features[~np.isnan(features).any(axis=1)]  # Remove rows with any NaN\n",
    "correlation_matrix = # Your code here\n",
    "\n",
    "# 5. Find features with highest variance (important for feature selection)\n",
    "feature_variances = # Your code here\n",
    "high_variance_features = # Your code here (top 3 indices)\n",
    "\n",
    "# 6. Detect outliers using z-score (|z| > 3)\n",
    "z_scores = # Your code here\n",
    "outlier_count = # Your code here\n",
    "\n",
    "# 7. Calculate skewness for each feature (measure of asymmetry)\n",
    "def skewness(x):\n",
    "    # Your code here - implement skewness formula\n",
    "    pass\n",
    "\n",
    "feature_skewness = # Your code here\n",
    "\n",
    "print(f\"Dataset shape: {features.shape}\")\n",
    "print(f\"Missing values: {np.isnan(features).sum()} ({np.isnan(features).mean()*100:.1f}%)\")\n",
    "print(f\"Feature means: {feature_means}\")\n",
    "print(f\"Feature variances: {feature_variances}\")\n",
    "print(f\"High variance features (indices): {high_variance_features}\")\n",
    "print(f\"Outliers detected: {outlier_count}\")\n",
    "print(f\"Correlation matrix shape: {correlation_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify statistical calculations\n",
    "print(f\"Feature means calculated: {len(feature_means) == 8}\")\n",
    "print(f\"No NaN in means: {not np.isnan(feature_means).any()}\")\n",
    "print(f\"Correlation matrix is symmetric: {np.allclose(correlation_matrix, correlation_matrix.T)}\")\n",
    "print(f\"Diagonal of correlation matrix is 1: {np.allclose(np.diag(correlation_matrix), 1)}\")\n",
    "print(f\"High variance features found: {len(high_variance_features) == 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-4",
   "metadata": {},
   "source": [
    "### 4. Broadcasting and Vectorization (Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Efficient computation without loops - critical for performance in ML pipelines.\n",
    "\n",
    "**Key concepts**: Broadcasting rules, vectorized operations, performance optimization\n",
    "\n",
    "**Common Questions**: \"How to normalize data efficiently?\" \"Compute distances between all pairs of points?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for ML preprocessing\n",
    "np.random.seed(789)\n",
    "X = np.random.randn(1000, 10)  # 1000 samples, 10 features\n",
    "y = np.random.choice([0, 1, 2], 1000)  # 3-class classification\n",
    "\n",
    "# Broadcasting and vectorization exercises\n",
    "\n",
    "# 1. Standardize features (zero mean, unit variance) using broadcasting\n",
    "X_standardized = # Your code here\n",
    "\n",
    "# 2. Min-Max normalization to [0, 1] range\n",
    "X_normalized = # Your code here\n",
    "\n",
    "# 3. Calculate pairwise Euclidean distances (first 100 samples for efficiency)\n",
    "X_subset = X[:100]\n",
    "distances = # Your code here - use broadcasting to avoid loops\n",
    "\n",
    "# 4. Apply different transformations to different features\n",
    "# Log transform to features 0-2, square root to features 3-5, leave others unchanged\n",
    "X_transformed = X.copy()\n",
    "# Your code here\n",
    "\n",
    "# 5. One-hot encode the target variable\n",
    "n_classes = len(np.unique(y))\n",
    "y_onehot = # Your code here\n",
    "\n",
    "# 6. Calculate class-wise feature means using broadcasting\n",
    "class_means = # Your code here\n",
    "\n",
    "# 7. Compute softmax probabilities (common in neural networks)\n",
    "logits = np.random.randn(100, 3)  # 100 samples, 3 classes\n",
    "def softmax(x):\n",
    "    # Your code here - implement numerically stable softmax\n",
    "    pass\n",
    "\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(f\"Original data - mean: {X.mean():.3f}, std: {X.std():.3f}\")\n",
    "print(f\"Standardized data - mean: {X_standardized.mean():.3f}, std: {X_standardized.std():.3f}\")\n",
    "print(f\"Normalized data - min: {X_normalized.min():.3f}, max: {X_normalized.max():.3f}\")\n",
    "print(f\"Distance matrix shape: {distances.shape}\")\n",
    "print(f\"One-hot encoded shape: {y_onehot.shape}\")\n",
    "print(f\"Class means shape: {class_means.shape}\")\n",
    "print(f\"Softmax probabilities sum to 1: {np.allclose(probabilities.sum(axis=1), 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify broadcasting operations\n",
    "print(f\"Standardization correct: {np.allclose(X_standardized.mean(axis=0), 0, atol=1e-10)}\")\n",
    "print(f\"Normalization correct: {X_normalized.min() >= 0 and X_normalized.max() <= 1}\")\n",
    "print(f\"Distance matrix symmetric: {np.allclose(distances, distances.T)}\")\n",
    "print(f\"Distance matrix diagonal is zero: {np.allclose(np.diag(distances), 0)}\")\n",
    "print(f\"One-hot encoding correct: {y_onehot.shape == (1000, 3)}\")\n",
    "print(f\"Softmax probabilities valid: {np.all(probabilities >= 0) and np.all(probabilities <= 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-5",
   "metadata": {},
   "source": [
    "### 5. Linear Algebra for Machine Learning (Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Linear algebra operations fundamental to ML algorithms.\n",
    "\n",
    "**Key concepts**: Matrix multiplication, eigenvalues, SVD, solving linear systems\n",
    "\n",
    "**Common Questions**: \"Implement PCA from scratch?\" \"Solve linear regression using normal equation?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear algebra operations for ML\n",
    "np.random.seed(101)\n",
    "\n",
    "# Create sample data for linear regression\n",
    "n_samples, n_features = 1000, 5\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_weights = np.array([1.5, -2.0, 0.5, 3.0, -1.0])\n",
    "noise = np.random.randn(n_samples) * 0.1\n",
    "y = X @ true_weights + noise\n",
    "\n",
    "# Add bias term\n",
    "X_with_bias = np.column_stack([np.ones(n_samples), X])\n",
    "\n",
    "# Linear algebra exercises\n",
    "\n",
    "# 1. Solve linear regression using normal equation: w = (X^T X)^(-1) X^T y\n",
    "weights_normal = # Your code here\n",
    "\n",
    "# 2. Compute eigenvalues and eigenvectors of covariance matrix\n",
    "cov_matrix = # Your code here\n",
    "eigenvalues, eigenvectors = # Your code here\n",
    "\n",
    "# 3. Perform SVD on the data matrix\n",
    "U, s, Vt = # Your code here\n",
    "\n",
    "# 4. Implement PCA transformation (reduce to 3 components)\n",
    "n_components = 3\n",
    "# Center the data first\n",
    "X_centered = # Your code here\n",
    "# Get principal components\n",
    "principal_components = # Your code here\n",
    "# Transform data\n",
    "X_pca = # Your code here\n",
    "\n",
    "# 5. Calculate matrix rank and condition number\n",
    "matrix_rank = # Your code here\n",
    "condition_number = # Your code here\n",
    "\n",
    "# 6. Compute QR decomposition\n",
    "Q, R = # Your code here\n",
    "\n",
    "# 7. Calculate determinant and trace\n",
    "det_cov = # Your code here\n",
    "trace_cov = # Your code here\n",
    "\n",
    "# 8. Compute matrix norms\n",
    "frobenius_norm = # Your code here\n",
    "spectral_norm = # Your code here\n",
    "\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"Estimated weights: {weights_normal[1:]}\")\n",
    "print(f\"Weight estimation error: {np.linalg.norm(weights_normal[1:] - true_weights):.6f}\")\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"SVD singular values: {s[:5]}\")\n",
    "print(f\"PCA transformed shape: {X_pca.shape}\")\n",
    "print(f\"Matrix rank: {matrix_rank}\")\n",
    "print(f\"Condition number: {condition_number:.2f}\")\n",
    "print(f\"Determinant: {det_cov:.6f}\")\n",
    "print(f\"Trace: {trace_cov:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify linear algebra operations\n",
    "print(f\"Normal equation solution close to true: {np.allclose(weights_normal[1:], true_weights, atol=0.1)}\")\n",
    "print(f\"Eigenvalues are real: {np.all(np.isreal(eigenvalues))}\")\n",
    "print(f\"SVD reconstruction works: {np.allclose(X_centered, U @ np.diag(s) @ Vt)}\")\n",
    "print(f\"PCA reduces dimensionality: {X_pca.shape[1] == n_components}\")\n",
    "print(f\"QR decomposition correct: {np.allclose(X_with_bias, Q @ R)}\")\n",
    "print(f\"Trace equals sum of eigenvalues: {np.allclose(trace_cov, eigenvalues.sum())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-6",
   "metadata": {},
   "source": [
    "### 6. Optimization and Gradient Computation (Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Implementing optimization algorithms from scratch - common in ML engineering roles.\n",
    "\n",
    "**Key concepts**: Gradient descent, cost functions, numerical optimization\n",
    "\n",
    "**Common Questions**: \"Implement gradient descent?\" \"How to compute gradients efficiently?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization algorithms for machine learning\n",
    "np.random.seed(202)\n",
    "\n",
    "# Generate dataset for logistic regression\n",
    "n_samples, n_features = 1000, 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_weights = np.array([0.5, -1.2, 0.8])\n",
    "logits = X @ true_weights\n",
    "probabilities = 1 / (1 + np.exp(-logits))  # Sigmoid\n",
    "y = np.random.binomial(1, probabilities)\n",
    "\n",
    "# Add bias term\n",
    "X_with_bias = np.column_stack([np.ones(n_samples), X])\n",
    "\n",
    "# Optimization functions\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Your code here - numerically stable sigmoid\n",
    "    pass\n",
    "\n",
    "def logistic_cost(weights, X, y):\n",
    "    # Your code here - logistic regression cost function\n",
    "    pass\n",
    "\n",
    "def logistic_gradient(weights, X, y):\n",
    "    # Your code here - gradient of logistic regression\n",
    "    pass\n",
    "\n",
    "def gradient_descent(X, y, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "    # Your code here - implement gradient descent\n",
    "    pass\n",
    "\n",
    "def stochastic_gradient_descent(X, y, learning_rate=0.01, max_iterations=1000, batch_size=32):\n",
    "    # Your code here - implement SGD with mini-batches\n",
    "    pass\n",
    "\n",
    "# Run optimizations\n",
    "weights_gd, costs_gd = gradient_descent(X_with_bias, y)\n",
    "weights_sgd, costs_sgd = stochastic_gradient_descent(X_with_bias, y)\n",
    "\n",
    "# Momentum-based gradient descent\n",
    "def gradient_descent_momentum(X, y, learning_rate=0.01, momentum=0.9, max_iterations=1000):\n",
    "    # Your code here - implement momentum\n",
    "    pass\n",
    "\n",
    "weights_momentum, costs_momentum = gradient_descent_momentum(X_with_bias, y)\n",
    "\n",
    "# Adam optimizer\n",
    "def adam_optimizer(X, y, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, max_iterations=1000):\n",
    "    # Your code here - implement Adam\n",
    "    pass\n",
    "\n",
    "weights_adam, costs_adam = adam_optimizer(X_with_bias, y)\n",
    "\n",
    "# Evaluate final models\n",
    "def accuracy(weights, X, y):\n",
    "    predictions = (sigmoid(X @ weights) > 0.5).astype(int)\n",
    "    return np.mean(predictions == y)\n",
    "\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"GD weights: {weights_gd[1:]}\")\n",
    "print(f\"SGD weights: {weights_sgd[1:]}\")\n",
    "print(f\"Momentum weights: {weights_momentum[1:]}\")\n",
    "print(f\"Adam weights: {weights_adam[1:]}\")\n",
    "print(f\"\\nAccuracies:\")\n",
    "print(f\"GD: {accuracy(weights_gd, X_with_bias, y):.4f}\")\n",
    "print(f\"SGD: {accuracy(weights_sgd, X_with_bias, y):.4f}\")\n",
    "print(f\"Momentum: {accuracy(weights_momentum, X_with_bias, y):.4f}\")\n",
    "print(f\"Adam: {accuracy(weights_adam, X_with_bias, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify optimization algorithms\n",
    "print(f\"GD converged: {costs_gd[-1] < costs_gd[0]}\")\n",
    "print(f\"SGD converged: {costs_sgd[-1] < costs_sgd[0]}\")\n",
    "print(f\"All accuracies > 0.8: {all(accuracy(w, X_with_bias, y) > 0.8 for w in [weights_gd, weights_sgd, weights_momentum, weights_adam])}\")\n",
    "print(f\"Weight estimates reasonable: {np.allclose(weights_gd[1:], true_weights, atol=0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-7",
   "metadata": {},
   "source": [
    "### 7. Feature Engineering and Data Preprocessing (Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Data preprocessing pipeline - essential for real-world ML projects.\n",
    "\n",
    "**Key concepts**: Feature scaling, polynomial features, binning, outlier handling\n",
    "\n",
    "**Common Questions**: \"How to handle different data types?\" \"Create interaction features?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for machine learning\n",
    "np.random.seed(303)\n",
    "\n",
    "# Create mixed-type dataset\n",
    "n_samples = 1000\n",
    "continuous_features = np.random.randn(n_samples, 3)\n",
    "categorical_features = np.random.choice(['A', 'B', 'C'], (n_samples, 2))\n",
    "ordinal_features = np.random.choice([1, 2, 3, 4, 5], (n_samples, 1))\n",
    "\n",
    "# Feature engineering functions\n",
    "\n",
    "# 1. Robust scaling (using median and IQR)\n",
    "def robust_scale(X):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "continuous_scaled = robust_scale(continuous_features)\n",
    "\n",
    "# 2. Create polynomial features (degree 2)\n",
    "def polynomial_features(X, degree=2):\n",
    "    # Your code here - include interaction terms\n",
    "    pass\n",
    "\n",
    "poly_features = polynomial_features(continuous_features[:, :2])  # Use first 2 features\n",
    "\n",
    "# 3. Binning continuous variables\n",
    "def create_bins(X, n_bins=5, strategy='quantile'):\n",
    "    # Your code here - quantile or uniform binning\n",
    "    pass\n",
    "\n",
    "binned_features = create_bins(continuous_features[:, 0])\n",
    "\n",
    "# 4. One-hot encoding for categorical variables\n",
    "def one_hot_encode(X):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "categorical_encoded = one_hot_encode(categorical_features[:, 0])\n",
    "\n",
    "# 5. Target encoding (mean encoding)\n",
    "# Create a target variable first\n",
    "target = np.random.binomial(1, 0.3, n_samples)\n",
    "\n",
    "def target_encode(categorical, target):\n",
    "    # Your code here - encode categories by target mean\n",
    "    pass\n",
    "\n",
    "target_encoded = target_encode(categorical_features[:, 0], target)\n",
    "\n",
    "# 6. Outlier detection and handling\n",
    "def detect_outliers_iqr(X, factor=1.5):\n",
    "    # Your code here - IQR method\n",
    "    pass\n",
    "\n",
    "def winsorize(X, limits=(0.05, 0.05)):\n",
    "    # Your code here - cap outliers at percentiles\n",
    "    pass\n",
    "\n",
    "outliers = detect_outliers_iqr(continuous_features[:, 0])\n",
    "winsorized = winsorize(continuous_features[:, 0])\n",
    "\n",
    "# 7. Feature selection using variance threshold\n",
    "def variance_threshold_selection(X, threshold=0.1):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# 8. Create interaction features\n",
    "def create_interactions(X):\n",
    "    # Your code here - pairwise products\n",
    "    pass\n",
    "\n",
    "interaction_features = create_interactions(continuous_features)\n",
    "\n",
    "# 9. Log transformation for skewed data\n",
    "skewed_data = np.random.exponential(2, (n_samples, 1))\n",
    "log_transformed = # Your code here\n",
    "\n",
    "print(f\"Original continuous features shape: {continuous_features.shape}\")\n",
    "print(f\"Polynomial features shape: {poly_features.shape}\")\n",
    "print(f\"Categorical encoded shape: {categorical_encoded.shape}\")\n",
    "print(f\"Interaction features shape: {interaction_features.shape}\")\n",
    "print(f\"Outliers detected: {np.sum(outliers)}\")\n",
    "print(f\"Original skewed data skewness: {((skewed_data - skewed_data.mean()) ** 3).mean() / skewed_data.std() ** 3:.2f}\")\n",
    "print(f\"Log-transformed skewness: {((log_transformed - log_transformed.mean()) ** 3).mean() / log_transformed.std() ** 3:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify feature engineering\n",
    "print(f\"Robust scaling worked: {np.abs(np.median(continuous_scaled, axis=0)).max() < 0.1}\")\n",
    "print(f\"Polynomial features include interactions: {poly_features.shape[1] > continuous_features.shape[1]}\")\n",
    "print(f\"One-hot encoding sums to 1: {np.allclose(categorical_encoded.sum(axis=1), 1)}\")\n",
    "print(f\"Outliers detected: {np.sum(outliers) > 0}\")\n",
    "print(f\"Log transformation reduces skewness: {abs(((log_transformed - log_transformed.mean()) ** 3).mean() / log_transformed.std() ** 3) < abs(((skewed_data - skewed_data.mean()) ** 3).mean() / skewed_data.std() ** 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-8",
   "metadata": {},
   "source": [
    "### 8. Time Series Analysis with NumPy (Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-8",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Time series preprocessing and feature extraction for ML models.\n",
    "\n",
    "**Key concepts**: Rolling windows, lag features, seasonal decomposition, trend analysis\n",
    "\n",
    "**Common Questions**: \"Create features for time series prediction?\" \"Handle temporal dependencies?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis and feature engineering\n",
    "np.random.seed(404)\n",
    "\n",
    "# Generate synthetic time series data\n",
    "n_points = 1000\n",
    "time = np.arange(n_points)\n",
    "\n",
    "# Create time series with trend, seasonality, and noise\n",
    "trend = 0.01 * time\n",
    "seasonal = 2 * np.sin(2 * np.pi * time / 50) + np.sin(2 * np.pi * time / 12)\n",
    "noise = np.random.randn(n_points) * 0.5\n",
    "ts_data = trend + seasonal + noise + 10  # Add baseline\n",
    "\n",
    "# Time series functions\n",
    "\n",
    "# 1. Moving averages (simple and exponential)\n",
    "def moving_average(data, window):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def exponential_moving_average(data, alpha=0.3):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "ma_5 = moving_average(ts_data, 5)\n",
    "ma_20 = moving_average(ts_data, 20)\n",
    "ema = exponential_moving_average(ts_data)\n",
    "\n",
    "# 2. Create lag features\n",
    "def create_lag_features(data, lags):\n",
    "    # Your code here - create matrix with lag features\n",
    "    pass\n",
    "\n",
    "lag_features = create_lag_features(ts_data, [1, 2, 3, 5, 10])\n",
    "\n",
    "# 3. Rolling statistics\n",
    "def rolling_statistics(data, window):\n",
    "    # Your code here - mean, std, min, max\n",
    "    pass\n",
    "\n",
    "rolling_stats = rolling_statistics(ts_data, 10)\n",
    "\n",
    "# 4. Difference features (for stationarity)\n",
    "def create_differences(data, orders=[1, 2]):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "diff_features = create_differences(ts_data)\n",
    "\n",
    "# 5. Seasonal decomposition (simplified)\n",
    "def seasonal_decompose_simple(data, period=50):\n",
    "    # Your code here - extract trend and seasonal components\n",
    "    pass\n",
    "\n",
    "trend_component, seasonal_component, residual = seasonal_decompose_simple(ts_data)\n",
    "\n",
    "# 6. Fourier features for seasonality\n",
    "def fourier_features(time, period, n_terms=3):\n",
    "    # Your code here - sin and cos features\n",
    "    pass\n",
    "\n",
    "fourier_feats = fourier_features(time, 50)\n",
    "\n",
    "# 7. Volatility features\n",
    "def calculate_volatility(data, window=20):\n",
    "    # Your code here - rolling standard deviation\n",
    "    pass\n",
    "\n",
    "volatility = calculate_volatility(ts_data)\n",
    "\n",
    "# 8. Change point detection (simple)\n",
    "def detect_change_points(data, window=20, threshold=2):\n",
    "    # Your code here - detect significant changes in mean\n",
    "    pass\n",
    "\n",
    "change_points = detect_change_points(ts_data)\n",
    "\n",
    "# 9. Autocorrelation features\n",
    "def autocorrelation(data, max_lag=20):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "autocorr = autocorrelation(ts_data)\n",
    "\n",
    "print(f\"Time series length: {len(ts_data)}\")\n",
    "print(f\"Moving averages shape: MA5={len(ma_5)}, MA20={len(ma_20)}\")\n",
    "print(f\"Lag features shape: {lag_features.shape}\")\n",
    "print(f\"Rolling statistics shape: {rolling_stats.shape}\")\n",
    "print(f\"Difference features shape: {diff_features.shape}\")\n",
    "print(f\"Fourier features shape: {fourier_feats.shape}\")\n",
    "print(f\"Change points detected: {np.sum(change_points)}\")\n",
    "print(f\"Autocorrelation shape: {autocorr.shape}\")\n",
    "print(f\"Max autocorrelation (lag 1): {autocorr[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify time series operations\n",
    "print(f\"Moving averages calculated: {len(ma_5) > 0 and len(ma_20) > 0}\")\n",
    "print(f\"Lag features have correct lags: {lag_features.shape[1] == 5}\")\n",
    "print(f\"Rolling stats include multiple metrics: {rolling_stats.shape[1] >= 4}\")\n",
    "print(f\"Seasonal decomposition sums correctly: {np.allclose(trend_component + seasonal_component + residual, ts_data, rtol=0.1)}\")\n",
    "print(f\"Fourier features are periodic: {fourier_feats.shape[1] >= 6}\")\n",
    "print(f\"Autocorrelation at lag 0 is 1: {np.isclose(autocorr[0], 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-9",
   "metadata": {},
   "source": [
    "### 9. Image Processing for Computer Vision (Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Basic image processing operations for CV/ML preprocessing.\n",
    "\n",
    "**Key concepts**: Convolution, filtering, feature extraction, image transformations\n",
    "\n",
    "**Common Questions**: \"Implement convolution from scratch?\" \"Extract features from images?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing with NumPy\n",
    "np.random.seed(505)\n",
    "\n",
    "# Create synthetic image data\n",
    "height, width = 64, 64\n",
    "image = np.random.rand(height, width)\n",
    "\n",
    "# Add some structure (circles and rectangles)\n",
    "y, x = np.ogrid[:height, :width]\n",
    "center_y, center_x = height // 2, width // 2\n",
    "circle_mask = (x - center_x) ** 2 + (y - center_y) ** 2 < 400\n",
    "image[circle_mask] = 0.8\n",
    "\n",
    "# Add rectangle\n",
    "image[10:20, 10:30] = 0.2\n",
    "\n",
    "# Image processing functions\n",
    "\n",
    "# 1. Convolution operation\n",
    "def convolve2d(image, kernel, padding='valid'):\n",
    "    # Your code here - implement 2D convolution\n",
    "    pass\n",
    "\n",
    "# Define common kernels\n",
    "sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
    "sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n",
    "gaussian_blur = np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16\n",
    "edge_detection = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])\n",
    "\n",
    "# Apply filters\n",
    "edges_x = convolve2d(image, sobel_x)\n",
    "edges_y = convolve2d(image, sobel_y)\n",
    "blurred = convolve2d(image, gaussian_blur)\n",
    "edges = convolve2d(image, edge_detection)\n",
    "\n",
    "# 2. Edge magnitude and direction\n",
    "edge_magnitude = # Your code here\n",
    "edge_direction = # Your code here\n",
    "\n",
    "# 3. Image gradients\n",
    "def image_gradients(image):\n",
    "    # Your code here - compute gradients using np.gradient\n",
    "    pass\n",
    "\n",
    "grad_y, grad_x = image_gradients(image)\n",
    "\n",
    "# 4. Local Binary Pattern (simplified)\n",
    "def local_binary_pattern(image, radius=1):\n",
    "    # Your code here - simplified LBP\n",
    "    pass\n",
    "\n",
    "lbp = local_binary_pattern(image)\n",
    "\n",
    "# 5. Histogram of Oriented Gradients (HOG) features\n",
    "def hog_features(image, cell_size=8, n_bins=9):\n",
    "    # Your code here - simplified HOG\n",
    "    pass\n",
    "\n",
    "hog = hog_features(image)\n",
    "\n",
    "# 6. Image moments\n",
    "def image_moments(image):\n",
    "    # Your code here - calculate spatial moments\n",
    "    pass\n",
    "\n",
    "moments = image_moments(image)\n",
    "\n",
    "# 7. Connected components (simplified)\n",
    "def connected_components(binary_image):\n",
    "    # Your code here - find connected regions\n",
    "    pass\n",
    "\n",
    "binary_image = image > 0.5\n",
    "components = connected_components(binary_image)\n",
    "\n",
    "# 8. Image statistics\n",
    "def image_statistics(image):\n",
    "    # Your code here - various statistical features\n",
    "    pass\n",
    "\n",
    "stats = image_statistics(image)\n",
    "\n",
    "print(f\"Original image shape: {image.shape}\")\n",
    "print(f\"Edge magnitude shape: {edge_magnitude.shape}\")\n",
    "print(f\"Gradient shapes: {grad_x.shape}, {grad_y.shape}\")\n",
    "print(f\"LBP shape: {lbp.shape}\")\n",
    "print(f\"HOG features length: {len(hog) if hog is not None else 'Not implemented'}\")\n",
    "print(f\"Image moments: {moments}\")\n",
    "print(f\"Connected components: {np.max(components) if components is not None else 'Not implemented'}\")\n",
    "print(f\"Image statistics: {stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify image processing operations\n",
    "print(f\"Convolution reduces image size: {edges_x.shape[0] < image.shape[0]}\")\n",
    "print(f\"Edge magnitude is non-negative: {np.all(edge_magnitude >= 0)}\")\n",
    "print(f\"Edge direction in valid range: {np.all((-np.pi <= edge_direction) & (edge_direction <= np.pi))}\")\n",
    "print(f\"Gradients computed: {grad_x.shape == image.shape and grad_y.shape == image.shape}\")\n",
    "print(f\"Image statistics calculated: {isinstance(stats, dict) and len(stats) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-10",
   "metadata": {},
   "source": [
    "### 10. Advanced NumPy Techniques and Performance (Mid-Advanced Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-10",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Advanced NumPy usage, memory optimization, and performance considerations.\n",
    "\n",
    "**Key concepts**: Memory views, advanced indexing, vectorization, numba integration\n",
    "\n",
    "**Common Questions**: \"Optimize NumPy code for large datasets?\" \"Memory-efficient operations?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced NumPy techniques\n",
    "import time\n",
    "np.random.seed(606)\n",
    "\n",
    "# Large dataset for performance testing\n",
    "large_data = np.random.randn(10000, 100)\n",
    "\n",
    "# Performance optimization techniques\n",
    "\n",
    "# 1. Memory views vs copies\n",
    "def demonstrate_views_vs_copies():\n",
    "    # Your code here - show difference between views and copies\n",
    "    pass\n",
    "\n",
    "# 2. Efficient array operations\n",
    "def efficient_operations(data):\n",
    "    # Your code here - use in-place operations, avoid temporary arrays\n",
    "    pass\n",
    "\n",
    "# 3. Advanced indexing techniques\n",
    "def advanced_indexing_examples(data):\n",
    "    # Your code here - fancy indexing, boolean masks, etc.\n",
    "    pass\n",
    "\n",
    "# 4. Vectorized string operations\n",
    "string_data = np.array(['apple', 'banana', 'cherry', 'date'] * 1000)\n",
    "def vectorized_string_ops(strings):\n",
    "    # Your code here - use np.char functions\n",
    "    pass\n",
    "\n",
    "# 5. Custom ufuncs\n",
    "def create_custom_ufunc():\n",
    "    # Your code here - create and use custom universal function\n",
    "    pass\n",
    "\n",
    "# 6. Memory-efficient operations for large arrays\n",
    "def memory_efficient_computation(data):\n",
    "    # Your code here - chunked processing, generators\n",
    "    pass\n",
    "\n",
    "# 7. Structured arrays for heterogeneous data\n",
    "def create_structured_array():\n",
    "    # Your code here - create and manipulate structured arrays\n",
    "    pass\n",
    "\n",
    "structured_data = create_structured_array()\n",
    "\n",
    "# 8. Performance comparison: loops vs vectorization\n",
    "def performance_comparison():\n",
    "    data = np.random.randn(100000)\n",
    "    \n",
    "    # Loop version\n",
    "    start_time = time.time()\n",
    "    result_loop = np.zeros_like(data)\n",
    "    for i in range(len(data)):\n",
    "        result_loop[i] = data[i] ** 2 + 2 * data[i] + 1\n",
    "    loop_time = time.time() - start_time\n",
    "    \n",
    "    # Vectorized version\n",
    "    start_time = time.time()\n",
    "    result_vectorized = # Your code here\n",
    "    vectorized_time = time.time() - start_time\n",
    "    \n",
    "    return loop_time, vectorized_time, np.allclose(result_loop, result_vectorized)\n",
    "\n",
    "loop_time, vec_time, results_match = performance_comparison()\n",
    "\n",
    "# 9. Advanced broadcasting examples\n",
    "def advanced_broadcasting():\n",
    "    # Your code here - complex broadcasting scenarios\n",
    "    pass\n",
    "\n",
    "# 10. Numerical stability considerations\n",
    "def numerical_stability_examples():\n",
    "    # Your code here - demonstrate numerical issues and solutions\n",
    "    pass\n",
    "\n",
    "stability_examples = numerical_stability_examples()\n",
    "\n",
    "print(f\"Large data shape: {large_data.shape}\")\n",
    "print(f\"Performance comparison:\")\n",
    "print(f\"  Loop time: {loop_time:.6f} seconds\")\n",
    "print(f\"  Vectorized time: {vec_time:.6f} seconds\")\n",
    "print(f\"  Speedup: {loop_time/vec_time:.1f}x\")\n",
    "print(f\"  Results match: {results_match}\")\n",
    "print(f\"Structured array created: {structured_data is not None}\")\n",
    "print(f\"Numerical stability examples: {stability_examples is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify advanced techniques\n",
    "print(f\"Vectorization is faster: {vec_time < loop_time}\")\n",
    "print(f\"Significant speedup achieved: {loop_time/vec_time > 5}\")\n",
    "print(f\"Large data processed: {large_data.size == 1000000}\")\n",
    "print(f\"String operations work: {len(string_data) == 4000}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3",
   "language": "python",
   "name": "python3.12.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
