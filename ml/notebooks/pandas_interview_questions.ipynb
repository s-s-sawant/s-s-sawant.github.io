{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pandas-intro",
   "metadata": {},
   "source": [
    "# Pandas for Data Science Interviews: 10 Essential Exercises\n",
    "\n",
    "## üêº Master Data Manipulation for ML/DS Roles\n",
    "\n",
    "Welcome to this comprehensive Pandas practice notebook designed specifically for data science and machine learning interviews! These exercises cover the most commonly asked Pandas concepts in technical interviews, from basic DataFrame operations to advanced data manipulation techniques used in real-world ML pipelines.\n",
    "\n",
    "### üìö What You'll Master\n",
    "- DataFrame creation, indexing, and manipulation\n",
    "- Data cleaning and preprocessing for ML\n",
    "- Groupby operations and aggregations\n",
    "- Time series analysis and feature engineering\n",
    "- Merging, joining, and data integration\n",
    "- Performance optimization for large datasets\n",
    "\n",
    "### üéØ Interview Focus Areas\n",
    "- **Entry Level**: Basic operations, filtering, simple aggregations\n",
    "- **Mid Level**: Complex groupby, merging, time series, data cleaning\n",
    "- **Advanced**: Performance optimization, custom functions, advanced transformations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-1",
   "metadata": {},
   "source": [
    "### 1. DataFrame Creation and Basic Operations (Entry Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Basic DataFrame creation and exploration - fundamental skill for any DS role.\n",
    "\n",
    "**Key concepts**: pd.DataFrame(), .info(), .describe(), .head(), .tail(), data types\n",
    "\n",
    "**Common Questions**: \"How do you explore a new dataset?\" \"Check data types and missing values?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create a sample customer dataset (common interview scenario)\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "customer_data = {\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'name': [f'Customer_{i}' for i in range(1, n_customers + 1)],\n",
    "    'age': np.random.randint(18, 80, n_customers),\n",
    "    'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_customers),\n",
    "    'signup_date': pd.date_range('2020-01-01', periods=n_customers, freq='D'),\n",
    "    'monthly_spend': np.random.uniform(50, 500, n_customers),\n",
    "    'is_premium': np.random.choice([True, False], n_customers, p=[0.3, 0.7])\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = # Your code here\n",
    "\n",
    "# Basic exploration tasks\n",
    "# 1. Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "# Your code here - show shape, dtypes, info\n",
    "\n",
    "# 2. Show first and last 5 rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "# Your code here\n",
    "\n",
    "print(\"\\nLast 5 rows:\")\n",
    "# Your code here\n",
    "\n",
    "# 3. Generate statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "# Your code here\n",
    "\n",
    "# 4. Check for missing values\n",
    "missing_values = # Your code here\n",
    "print(f\"\\nMissing values per column:\\n{missing_values}\")\n",
    "\n",
    "# 5. Get unique values for categorical columns\n",
    "unique_cities = # Your code here\n",
    "print(f\"\\nUnique cities: {unique_cities}\")\n",
    "\n",
    "# 6. Basic filtering\n",
    "premium_customers = # Your code here - filter premium customers\n",
    "print(f\"\\nPremium customers: {len(premium_customers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify DataFrame creation and basic operations\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Correct number of customers: {len(df) == n_customers}\")\n",
    "print(f\"Premium customers filtered correctly: {len(premium_customers) == df['is_premium'].sum()}\")\n",
    "print(f\"No missing values: {df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning and Missing Value Handling (Entry-Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Data cleaning pipeline - essential for real-world ML projects.\n",
    "\n",
    "**Key concepts**: .isnull(), .dropna(), .fillna(), .drop_duplicates(), data validation\n",
    "\n",
    "**Common Questions**: \"How do you handle missing data?\" \"Clean messy datasets for ML?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messy dataset (common in real-world scenarios)\n",
    "np.random.seed(123)\n",
    "messy_data = pd.DataFrame({\n",
    "    'employee_id': range(1, 501),\n",
    "    'name': [f'Employee_{i}' if i % 10 != 0 else None for i in range(1, 501)],\n",
    "    'department': np.random.choice(['IT', 'Sales', 'Marketing', 'HR', None], 500, p=[0.3, 0.25, 0.2, 0.2, 0.05]),\n",
    "    'salary': [np.random.normal(60000, 15000) if i % 15 != 0 else None for i in range(500)],\n",
    "    'hire_date': pd.date_range('2015-01-01', periods=500, freq='D'),\n",
    "    'email': [f'emp{i}@company.com' if i % 8 != 0 else '' for i in range(1, 501)],\n",
    "    'performance_score': np.random.uniform(1, 5, 500)\n",
    "})\n",
    "\n",
    "# Add some duplicates\n",
    "duplicates = messy_data.sample(20).copy()\n",
    "messy_data = pd.concat([messy_data, duplicates], ignore_index=True)\n",
    "\n",
    "# Add some invalid data\n",
    "messy_data.loc[messy_data.index % 20 == 0, 'salary'] = -1000  # Invalid negative salaries\n",
    "\n",
    "print(\"Original messy data:\")\n",
    "print(f\"Shape: {messy_data.shape}\")\n",
    "print(f\"Missing values:\\n{messy_data.isnull().sum()}\")\n",
    "print(f\"Duplicates: {messy_data.duplicated().sum()}\")\n",
    "\n",
    "# Data cleaning pipeline\n",
    "cleaned_data = messy_data.copy()\n",
    "\n",
    "# 1. Remove exact duplicates\n",
    "cleaned_data = # Your code here\n",
    "\n",
    "# 2. Handle missing names (fill with 'Unknown')\n",
    "# Your code here\n",
    "\n",
    "# 3. Handle missing departments (fill with most common department)\n",
    "most_common_dept = # Your code here\n",
    "# Your code here - fill missing departments\n",
    "\n",
    "# 4. Handle missing salaries (fill with department median)\n",
    "# Your code here\n",
    "\n",
    "# 5. Clean email column (replace empty strings with NaN, then fill)\n",
    "# Your code here\n",
    "\n",
    "# 6. Handle invalid salaries (negative values)\n",
    "# Your code here - replace with department median\n",
    "\n",
    "# 7. Validate data types\n",
    "# Your code here - ensure salary is numeric, dates are datetime\n",
    "\n",
    "# 8. Create data quality report\n",
    "def data_quality_report(df):\n",
    "    # Your code here - return dict with quality metrics\n",
    "    pass\n",
    "\n",
    "quality_report = data_quality_report(cleaned_data)\n",
    "\n",
    "print(\"\\nCleaned data:\")\n",
    "print(f\"Shape: {cleaned_data.shape}\")\n",
    "print(f\"Missing values: {cleaned_data.isnull().sum().sum()}\")\n",
    "print(f\"Duplicates: {cleaned_data.duplicated().sum()}\")\n",
    "print(f\"Data quality report: {quality_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify data cleaning\n",
    "print(f\"No duplicates: {cleaned_data.duplicated().sum() == 0}\")\n",
    "print(f\"No missing names: {cleaned_data['name'].isnull().sum() == 0}\")\n",
    "print(f\"No missing departments: {cleaned_data['department'].isnull().sum() == 0}\")\n",
    "print(f\"No negative salaries: {(cleaned_data['salary'] < 0).sum() == 0}\")\n",
    "print(f\"All emails valid: {(cleaned_data['email'] == '').sum() == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3",
   "metadata": {},
   "source": [
    "### 3. Advanced Filtering and Conditional Operations (Entry-Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Complex data selection and filtering - critical for feature engineering.\n",
    "\n",
    "**Key concepts**: Boolean indexing, .query(), .loc[], .isin(), multiple conditions\n",
    "\n",
    "**Common Questions**: \"Filter data based on multiple conditions?\" \"Select specific subsets for analysis?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sales dataset for filtering exercises\n",
    "np.random.seed(456)\n",
    "sales_data = pd.DataFrame({\n",
    "    'transaction_id': range(1, 10001),\n",
    "    'customer_id': np.random.randint(1, 1001, 10000),\n",
    "    'product_category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home', 'Sports'], 10000),\n",
    "    'product_name': [f'Product_{i}' for i in np.random.randint(1, 501, 10000)],\n",
    "    'quantity': np.random.randint(1, 10, 10000),\n",
    "    'unit_price': np.random.uniform(10, 500, 10000),\n",
    "    'discount': np.random.uniform(0, 0.3, 10000),\n",
    "    'sales_rep': np.random.choice([f'Rep_{i}' for i in range(1, 21)], 10000),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], 10000),\n",
    "    'transaction_date': pd.date_range('2024-01-01', periods=10000, freq='H')\n",
    "})\n",
    "\n",
    "# Calculate total amount\n",
    "sales_data['total_amount'] = # Your code here\n",
    "\n",
    "# Advanced filtering exercises\n",
    "\n",
    "# 1. High-value transactions (>$1000) in Electronics or Clothing\n",
    "high_value_tech_clothing = # Your code here\n",
    "\n",
    "# 2. Transactions with high discount (>20%) but low quantity (<3)\n",
    "high_discount_low_qty = # Your code here\n",
    "\n",
    "# 3. Top 10% of transactions by total amount\n",
    "top_10_percent = # Your code here\n",
    "\n",
    "# 4. Transactions from specific sales reps in specific regions\n",
    "target_reps = ['Rep_1', 'Rep_5', 'Rep_10']\n",
    "target_regions = ['North', 'East']\n",
    "filtered_reps_regions = # Your code here\n",
    "\n",
    "# 5. Using query method for complex conditions\n",
    "query_result = # Your code here - use .query() for multiple conditions\n",
    "\n",
    "# 6. Transactions in the last 30 days of the dataset\n",
    "last_30_days = # Your code here\n",
    "\n",
    "# 7. Outlier detection - transactions with unusual patterns\n",
    "def detect_outliers(df, column, method='iqr'):\n",
    "    # Your code here - implement IQR or z-score method\n",
    "    pass\n",
    "\n",
    "outlier_transactions = detect_outliers(sales_data, 'total_amount')\n",
    "\n",
    "# 8. Complex conditional assignment\n",
    "def assign_customer_tier(row):\n",
    "    # Your code here - assign tier based on total_amount and quantity\n",
    "    pass\n",
    "\n",
    "sales_data['customer_tier'] = # Your code here\n",
    "\n",
    "# 9. Multi-level filtering with aggregation\n",
    "high_performing_reps = # Your code here - reps with avg transaction > $200\n",
    "\n",
    "print(f\"Total transactions: {len(sales_data)}\")\n",
    "print(f\"High-value tech/clothing: {len(high_value_tech_clothing)}\")\n",
    "print(f\"High discount, low quantity: {len(high_discount_low_qty)}\")\n",
    "print(f\"Top 10% transactions: {len(top_10_percent)}\")\n",
    "print(f\"Filtered reps/regions: {len(filtered_reps_regions)}\")\n",
    "print(f\"Query result: {len(query_result)}\")\n",
    "print(f\"Last 30 days: {len(last_30_days)}\")\n",
    "print(f\"Outlier transactions: {len(outlier_transactions)}\")\n",
    "print(f\"Customer tier distribution:\\n{sales_data['customer_tier'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify filtering operations\n",
    "print(f\"High-value filter works: {(high_value_tech_clothing['total_amount'] > 1000).all()}\")\n",
    "print(f\"High discount filter works: {(high_discount_low_qty['discount'] > 0.2).all() and (high_discount_low_qty['quantity'] < 3).all()}\")\n",
    "print(f\"Top 10% correct size: {len(top_10_percent) == len(sales_data) // 10}\")\n",
    "print(f\"Date filtering works: {len(last_30_days) > 0}\")\n",
    "print(f\"Customer tiers assigned: {'customer_tier' in sales_data.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-4",
   "metadata": {},
   "source": [
    "### 4. GroupBy Operations and Aggregations (Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Data aggregation and summarization - core skill for data analysis.\n",
    "\n",
    "**Key concepts**: .groupby(), .agg(), custom aggregation functions, multiple grouping levels\n",
    "\n",
    "**Common Questions**: \"Analyze sales by different dimensions?\" \"Create business metrics?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the sales_data from previous exercise\n",
    "# GroupBy and aggregation exercises\n",
    "\n",
    "# 1. Sales summary by product category\n",
    "category_summary = # Your code here - multiple aggregations\n",
    "\n",
    "# 2. Performance by sales rep and region\n",
    "rep_region_performance = # Your code here\n",
    "\n",
    "# 3. Monthly sales trends\n",
    "sales_data['month'] = sales_data['transaction_date'].dt.to_period('M')\n",
    "monthly_trends = # Your code here\n",
    "\n",
    "# 4. Custom aggregation functions\n",
    "def coefficient_of_variation(x):\n",
    "    # Your code here - return std/mean\n",
    "    pass\n",
    "\n",
    "def transaction_efficiency(group):\n",
    "    # Your code here - total_amount / quantity ratio\n",
    "    pass\n",
    "\n",
    "custom_agg = # Your code here - use custom functions\n",
    "\n",
    "# 5. Multi-level grouping with different aggregations per column\n",
    "multi_level_agg = sales_data.groupby(['region', 'product_category']).agg({\n",
    "    # Your code here - different agg functions for different columns\n",
    "})\n",
    "\n",
    "# 6. Rolling aggregations by group\n",
    "rolling_sales = # Your code here - 7-day rolling average by sales_rep\n",
    "\n",
    "# 7. Rank within groups\n",
    "sales_data['rank_in_category'] = # Your code here - rank by total_amount within category\n",
    "\n",
    "# 8. Group filtering\n",
    "high_volume_categories = # Your code here - categories with >1000 transactions\n",
    "\n",
    "# 9. Transform operations\n",
    "sales_data['pct_of_category_total'] = # Your code here - percentage of category total\n",
    "\n",
    "# 10. Advanced groupby with apply\n",
    "def top_products_per_category(group, n=3):\n",
    "    # Your code here - return top n products by total sales\n",
    "    pass\n",
    "\n",
    "top_products = # Your code here\n",
    "\n",
    "print(\"Category Summary:\")\n",
    "print(category_summary.head())\n",
    "print(\"\\nRep-Region Performance:\")\n",
    "print(rep_region_performance.head())\n",
    "print(\"\\nMonthly Trends:\")\n",
    "print(monthly_trends.head())\n",
    "print(\"\\nCustom Aggregations:\")\n",
    "print(custom_agg.head())\n",
    "print(f\"\\nHigh Volume Categories: {len(high_volume_categories)}\")\n",
    "print(f\"Top Products per Category: {len(top_products)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify groupby operations\n",
    "print(f\"Category summary has all categories: {len(category_summary) == sales_data['product_category'].nunique()}\")\n",
    "print(f\"Monthly trends calculated: {len(monthly_trends) > 0}\")\n",
    "print(f\"Ranks assigned: {'rank_in_category' in sales_data.columns}\")\n",
    "print(f\"Percentages calculated: {'pct_of_category_total' in sales_data.columns}\")\n",
    "print(f\"Percentages sum to 100 per category: {np.allclose(sales_data.groupby('product_category')['pct_of_category_total'].sum(), 100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-5",
   "metadata": {},
   "source": [
    "### 5. Time Series Analysis and Date Operations (Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Time series manipulation for feature engineering and analysis.\n",
    "\n",
    "**Key concepts**: DatetimeIndex, .resample(), .shift(), time-based grouping, lag features\n",
    "\n",
    "**Common Questions**: \"Create time-based features?\" \"Analyze temporal patterns?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series dataset\n",
    "np.random.seed(789)\n",
    "date_range = pd.date_range('2022-01-01', '2024-12-31', freq='D')\n",
    "n_days = len(date_range)\n",
    "\n",
    "# Simulate stock/sales data with seasonality and trend\n",
    "trend = np.linspace(100, 150, n_days)\n",
    "seasonal = 10 * np.sin(2 * np.pi * np.arange(n_days) / 365.25)\n",
    "weekly = 5 * np.sin(2 * np.pi * np.arange(n_days) / 7)\n",
    "noise = np.random.normal(0, 5, n_days)\n",
    "values = trend + seasonal + weekly + noise\n",
    "\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': date_range,\n",
    "    'value': values,\n",
    "    'volume': np.random.randint(1000, 10000, n_days),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], n_days)\n",
    "})\n",
    "\n",
    "ts_data.set_index('date', inplace=True)\n",
    "\n",
    "# Time series operations\n",
    "\n",
    "# 1. Extract time components\n",
    "ts_data['year'] = # Your code here\n",
    "ts_data['month'] = # Your code here\n",
    "ts_data['day_of_week'] = # Your code here\n",
    "ts_data['quarter'] = # Your code here\n",
    "ts_data['is_weekend'] = # Your code here\n",
    "\n",
    "# 2. Create lag features\n",
    "for lag in [1, 7, 30]:\n",
    "    ts_data[f'value_lag_{lag}'] = # Your code here\n",
    "\n",
    "# 3. Rolling statistics\n",
    "windows = [7, 30, 90]\n",
    "for window in windows:\n",
    "    ts_data[f'rolling_mean_{window}'] = # Your code here\n",
    "    ts_data[f'rolling_std_{window}'] = # Your code here\n",
    "\n",
    "# 4. Percentage change and returns\n",
    "ts_data['pct_change_1d'] = # Your code here\n",
    "ts_data['pct_change_7d'] = # Your code here\n",
    "\n",
    "# 5. Resample to different frequencies\n",
    "weekly_data = # Your code here - resample to weekly\n",
    "monthly_data = # Your code here - resample to monthly\n",
    "\n",
    "# 6. Seasonal decomposition (simplified)\n",
    "def simple_seasonal_decompose(series, period=365):\n",
    "    # Your code here - extract trend and seasonal components\n",
    "    pass\n",
    "\n",
    "decomp_result = simple_seasonal_decompose(ts_data['value'])\n",
    "\n",
    "# 7. Time-based filtering\n",
    "recent_data = # Your code here - last 90 days\n",
    "business_hours = # Your code here - filter by hour if applicable\n",
    "specific_months = # Your code here - Q4 data (Oct, Nov, Dec)\n",
    "\n",
    "# 8. Forward fill and backward fill for missing values\n",
    "ts_data_with_gaps = ts_data.copy()\n",
    "ts_data_with_gaps.loc[ts_data_with_gaps.index[::100], 'value'] = np.nan\n",
    "forward_filled = # Your code here\n",
    "backward_filled = # Your code here\n",
    "\n",
    "# 9. Time-based groupby operations\n",
    "monthly_stats = # Your code here - group by month and calculate stats\n",
    "day_of_week_patterns = # Your code here - analyze day-of-week patterns\n",
    "\n",
    "# 10. Create cyclical features\n",
    "ts_data['month_sin'] = # Your code here - sin encoding for month\n",
    "ts_data['month_cos'] = # Your code here - cos encoding for month\n",
    "ts_data['day_sin'] = # Your code here - sin encoding for day of week\n",
    "ts_data['day_cos'] = # Your code here - cos encoding for day of week\n",
    "\n",
    "print(f\"Time series data shape: {ts_data.shape}\")\n",
    "print(f\"Date range: {ts_data.index.min()} to {ts_data.index.max()}\")\n",
    "print(f\"Weekly data shape: {weekly_data.shape}\")\n",
    "print(f\"Monthly data shape: {monthly_data.shape}\")\n",
    "print(f\"Recent data (90 days): {len(recent_data)}\")\n",
    "print(f\"Q4 data: {len(specific_months)}\")\n",
    "print(\"\\nTime series features created:\")\n",
    "print(ts_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify time series operations\n",
    "print(f\"Time components extracted: {all(col in ts_data.columns for col in ['year', 'month', 'day_of_week', 'quarter'])}\")\n",
    "print(f\"Lag features created: {all(f'value_lag_{lag}' in ts_data.columns for lag in [1, 7, 30])}\")\n",
    "print(f\"Rolling features created: {all(f'rolling_mean_{w}' in ts_data.columns for w in [7, 30, 90])}\")\n",
    "print(f\"Percentage changes calculated: {'pct_change_1d' in ts_data.columns}\")\n",
    "print(f\"Cyclical features created: {all(col in ts_data.columns for col in ['month_sin', 'month_cos', 'day_sin', 'day_cos'])}\")\n",
    "print(f\"Resampling worked: {len(weekly_data) < len(ts_data) and len(monthly_data) < len(weekly_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-6",
   "metadata": {},
   "source": [
    "### 6. Data Merging and Joining Operations (Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Combining datasets from multiple sources - essential for real-world projects.\n",
    "\n",
    "**Key concepts**: .merge(), .join(), different join types, handling merge conflicts\n",
    "\n",
    "**Common Questions**: \"Combine customer and transaction data?\" \"Handle many-to-many relationships?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple related datasets (e-commerce scenario)\n",
    "np.random.seed(101)\n",
    "\n",
    "# Customers table\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': range(1, 1001),\n",
    "    'customer_name': [f'Customer_{i}' for i in range(1, 1001)],\n",
    "    'email': [f'customer{i}@email.com' for i in range(1, 1001)],\n",
    "    'registration_date': pd.date_range('2020-01-01', periods=1000, freq='D'),\n",
    "    'city': np.random.choice(['NYC', 'LA', 'Chicago', 'Houston'], 1000),\n",
    "    'customer_segment': np.random.choice(['Premium', 'Standard', 'Basic'], 1000, p=[0.2, 0.5, 0.3])\n",
    "})\n",
    "\n",
    "# Orders table\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': range(1, 5001),\n",
    "    'customer_id': np.random.choice(range(1, 1001), 5000),\n",
    "    'order_date': pd.date_range('2023-01-01', periods=5000, freq='2H'),\n",
    "    'total_amount': np.random.uniform(20, 500, 5000),\n",
    "    'status': np.random.choice(['Completed', 'Pending', 'Cancelled'], 5000, p=[0.8, 0.15, 0.05]),\n",
    "    'shipping_cost': np.random.uniform(5, 25, 5000)\n",
    "})\n",
    "\n",
    "# Products table\n",
    "products = pd.DataFrame({\n",
    "    'product_id': range(1, 201),\n",
    "    'product_name': [f'Product_{i}' for i in range(1, 201)],\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], 200),\n",
    "    'price': np.random.uniform(10, 300, 200),\n",
    "    'supplier_id': np.random.randint(1, 21, 200)\n",
    "})\n",
    "\n",
    "# Order items table (many-to-many relationship)\n",
    "order_items = pd.DataFrame({\n",
    "    'order_id': np.random.choice(range(1, 5001), 15000),\n",
    "    'product_id': np.random.choice(range(1, 201), 15000),\n",
    "    'quantity': np.random.randint(1, 5, 15000),\n",
    "    'unit_price': np.random.uniform(10, 300, 15000)\n",
    "})\n",
    "\n",
    "# Suppliers table\n",
    "suppliers = pd.DataFrame({\n",
    "    'supplier_id': range(1, 21),\n",
    "    'supplier_name': [f'Supplier_{i}' for i in range(1, 21)],\n",
    "    'country': np.random.choice(['USA', 'China', 'Germany', 'Japan'], 20),\n",
    "    'rating': np.random.uniform(3, 5, 20)\n",
    "})\n",
    "\n",
    "# Merging exercises\n",
    "\n",
    "# 1. Customer order summary\n",
    "customer_orders = # Your code here - merge customers with orders\n",
    "\n",
    "# 2. Complete order details with customer info\n",
    "complete_orders = # Your code here - orders + customers + order_items + products\n",
    "\n",
    "# 3. Find customers who never placed an order\n",
    "customers_no_orders = # Your code here - use indicator parameter\n",
    "\n",
    "# 4. Product sales analysis with supplier info\n",
    "product_sales = # Your code here - aggregate order_items, merge with products and suppliers\n",
    "\n",
    "# 5. Customer lifetime value calculation\n",
    "customer_ltv = # Your code here - calculate total spent per customer\n",
    "\n",
    "# 6. Handle duplicate keys in merge\n",
    "# Create scenario with duplicate customer records\n",
    "customers_with_dupes = pd.concat([customers, customers.sample(50)], ignore_index=True)\n",
    "merge_with_dupes = # Your code here - handle duplicates appropriately\n",
    "\n",
    "# 7. Time-based joins\n",
    "# Join orders with customer registration to find time since registration\n",
    "orders_with_tenure = # Your code here\n",
    "\n",
    "# 8. Cross join for recommendation matrix (sample)\n",
    "sample_customers = customers.sample(10)\n",
    "sample_products = products.sample(20)\n",
    "recommendation_matrix = # Your code here - cross join\n",
    "\n",
    "# 9. Fuzzy matching (simplified)\n",
    "def fuzzy_merge_example():\n",
    "    # Your code here - demonstrate handling slight name variations\n",
    "    pass\n",
    "\n",
    "# 10. Validate merge results\n",
    "def validate_merge_results(original_df, merged_df, join_type):\n",
    "    # Your code here - check for data loss, duplicates, etc.\n",
    "    pass\n",
    "\n",
    "validation_results = validate_merge_results(orders, customer_orders, 'left')\n",
    "\n",
    "print(f\"Customers: {len(customers)}, Orders: {len(orders)}\")\n",
    "print(f\"Customer orders: {len(customer_orders)}\")\n",
    "print(f\"Complete orders: {len(complete_orders)}\")\n",
    "print(f\"Customers with no orders: {len(customers_no_orders)}\")\n",
    "print(f\"Product sales analysis: {len(product_sales)}\")\n",
    "print(f\"Customer LTV: {len(customer_ltv)}\")\n",
    "print(f\"Recommendation matrix: {recommendation_matrix.shape}\")\n",
    "print(f\"Merge validation: {validation_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify merge operations\n",
    "print(f\"Customer orders preserves all orders: {len(customer_orders) == len(orders)}\")\n",
    "print(f\"Complete orders has customer info: {'customer_name' in complete_orders.columns}\")\n",
    "print(f\"Customers without orders identified: {len(customers_no_orders) > 0}\")\n",
    "print(f\"Product sales has supplier info: {'supplier_name' in product_sales.columns}\")\n",
    "print(f\"Customer LTV calculated: {'total_spent' in customer_ltv.columns}\")\n",
    "print(f\"Cross join creates all combinations: {len(recommendation_matrix) == len(sample_customers) * len(sample_products)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-7",
   "metadata": {},
   "source": [
    "### 7. String Operations and Text Processing (Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Text data preprocessing for NLP and feature extraction.\n",
    "\n",
    "**Key concepts**: .str accessor, regex operations, text cleaning, feature extraction\n",
    "\n",
    "**Common Questions**: \"Clean text data for analysis?\" \"Extract information from unstructured text?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text dataset for processing\n",
    "np.random.seed(202)\n",
    "\n",
    "# Customer feedback data\n",
    "feedback_data = pd.DataFrame({\n",
    "    'review_id': range(1, 1001),\n",
    "    'customer_email': [f'customer{i}@{domain}.com' for i, domain in \n",
    "                      enumerate(np.random.choice(['gmail', 'yahoo', 'hotmail', 'outlook'], 1000))],\n",
    "    'phone': [f'+1-{np.random.randint(100,999)}-{np.random.randint(100,999)}-{np.random.randint(1000,9999)}' \n",
    "             for _ in range(1000)],\n",
    "    'review_text': [\n",
    "        \"Great product! Really satisfied with the quality. 5/5 stars!\",\n",
    "        \"Poor service, took 2 weeks to deliver. Rating: 2/5\",\n",
    "        \"Amazing experience! Fast delivery in 3 days. Highly recommend! ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\",\n",
    "        \"Product was damaged. Customer service was helpful though. 3/5\",\n",
    "        \"Excellent quality and fast shipping! Will buy again. 5 stars!\",\n",
    "        \"Not worth the price. Expected better quality. 2/5 rating\",\n",
    "        \"Perfect! Arrived in 1 day. Great packaging. 5/5!\",\n",
    "        \"Average product. Nothing special. 3 out of 5 stars\",\n",
    "        \"Terrible experience. Wrong item delivered. 1/5\",\n",
    "        \"Good value for money. Delivery took 5 days. 4/5\"\n",
    "    ] * 100,\n",
    "    'product_name': [f'Product {name}' for name in \n",
    "                    np.random.choice(['iPhone 15', 'Samsung TV', 'Nike Shoes', 'Dell Laptop', 'Sony Headphones'], 1000)],\n",
    "    'review_date': pd.date_range('2024-01-01', periods=1000, freq='6H')\n",
    "})\n",
    "\n",
    "# String processing exercises\n",
    "\n",
    "# 1. Extract email domains\n",
    "feedback_data['email_domain'] = # Your code here\n",
    "\n",
    "# 2. Clean and standardize phone numbers\n",
    "feedback_data['phone_clean'] = # Your code here - remove all non-digits\n",
    "\n",
    "# 3. Extract ratings from review text\n",
    "def extract_rating(text):\n",
    "    # Your code here - extract X/5 or X out of 5 patterns\n",
    "    pass\n",
    "\n",
    "feedback_data['extracted_rating'] = # Your code here\n",
    "\n",
    "# 4. Extract delivery time mentions\n",
    "def extract_delivery_days(text):\n",
    "    # Your code here - extract \"X days\" patterns\n",
    "    pass\n",
    "\n",
    "feedback_data['delivery_days'] = # Your code here\n",
    "\n",
    "# 5. Sentiment classification based on keywords\n",
    "positive_words = ['great', 'excellent', 'amazing', 'perfect', 'good', 'satisfied', 'recommend']\n",
    "negative_words = ['poor', 'terrible', 'damaged', 'wrong', 'bad', 'awful']\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    # Your code here - classify as positive/negative/neutral\n",
    "    pass\n",
    "\n",
    "feedback_data['sentiment'] = # Your code here\n",
    "\n",
    "# 6. Text cleaning and preprocessing\n",
    "def clean_text(text):\n",
    "    # Your code here - lowercase, remove punctuation, etc.\n",
    "    pass\n",
    "\n",
    "feedback_data['cleaned_text'] = # Your code here\n",
    "\n",
    "# 7. Word count and text statistics\n",
    "feedback_data['word_count'] = # Your code here\n",
    "feedback_data['char_count'] = # Your code here\n",
    "feedback_data['exclamation_count'] = # Your code here\n",
    "\n",
    "# 8. Extract product brand from product name\n",
    "def extract_brand(product_name):\n",
    "    # Your code here - extract first word as brand\n",
    "    pass\n",
    "\n",
    "feedback_data['brand'] = # Your code here\n",
    "\n",
    "# 9. Find reviews mentioning specific keywords\n",
    "delivery_mentions = # Your code here - reviews mentioning 'delivery' or 'shipping'\n",
    "quality_mentions = # Your code here - reviews mentioning 'quality'\n",
    "price_mentions = # Your code here - reviews mentioning 'price' or 'value'\n",
    "\n",
    "# 10. Create text features for ML\n",
    "def create_text_features(df):\n",
    "    # Your code here - create various text-based features\n",
    "    pass\n",
    "\n",
    "text_features = create_text_features(feedback_data)\n",
    "\n",
    "# 11. Handle text encoding issues\n",
    "def fix_encoding_issues(text):\n",
    "    # Your code here - handle common encoding problems\n",
    "    pass\n",
    "\n",
    "# 12. Extract structured information\n",
    "contact_info = feedback_data['customer_email'] + ' | ' + feedback_data['phone']\n",
    "extracted_contacts = # Your code here - split back into components\n",
    "\n",
    "print(\"Text Processing Results:\")\n",
    "print(f\"Email domains: {feedback_data['email_domain'].value_counts().head()}\")\n",
    "print(f\"\\nSentiment distribution: {feedback_data['sentiment'].value_counts()}\")\n",
    "print(f\"\\nRatings extracted: {feedback_data['extracted_rating'].value_counts()}\")\n",
    "print(f\"\\nDelivery mentions: {len(delivery_mentions)}\")\n",
    "print(f\"Quality mentions: {len(quality_mentions)}\")\n",
    "print(f\"Price mentions: {len(price_mentions)}\")\n",
    "print(f\"\\nBrand distribution: {feedback_data['brand'].value_counts()}\")\n",
    "print(f\"\\nText statistics - avg word count: {feedback_data['word_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify string operations\n",
    "print(f\"Email domains extracted: {feedback_data['email_domain'].notna().sum() > 0}\")\n",
    "print(f\"Phone numbers cleaned: {feedback_data['phone_clean'].str.isdigit().sum() > 0}\")\n",
    "print(f\"Ratings extracted: {feedback_data['extracted_rating'].notna().sum() > 0}\")\n",
    "print(f\"Sentiment classified: {feedback_data['sentiment'].notna().sum() > 0}\")\n",
    "print(f\"Text statistics calculated: {all(col in feedback_data.columns for col in ['word_count', 'char_count'])}\")\n",
    "print(f\"Brands extracted: {feedback_data['brand'].notna().sum() > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-8",
   "metadata": {},
   "source": [
    "### 8. Pivot Tables and Data Reshaping (Mid Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-8",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Data reshaping for analysis and reporting - common in business analytics.\n",
    "\n",
    "**Key concepts**: .pivot_table(), .melt(), .stack()/.unstack(), wide vs long format\n",
    "\n",
    "**Common Questions**: \"Create summary reports?\" \"Reshape data for different analyses?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wide-format dataset for reshaping\n",
    "np.random.seed(303)\n",
    "\n",
    "# Student performance data\n",
    "students = pd.DataFrame({\n",
    "    'student_id': range(1, 201),\n",
    "    'name': [f'Student_{i}' for i in range(1, 201)],\n",
    "    'grade_level': np.random.choice(['9th', '10th', '11th', '12th'], 200),\n",
    "    'school': np.random.choice(['School_A', 'School_B', 'School_C'], 200),\n",
    "    'math_q1': np.random.randint(60, 100, 200),\n",
    "    'math_q2': np.random.randint(60, 100, 200),\n",
    "    'math_q3': np.random.randint(60, 100, 200),\n",
    "    'math_q4': np.random.randint(60, 100, 200),\n",
    "    'science_q1': np.random.randint(60, 100, 200),\n",
    "    'science_q2': np.random.randint(60, 100, 200),\n",
    "    'science_q3': np.random.randint(60, 100, 200),\n",
    "    'science_q4': np.random.randint(60, 100, 200),\n",
    "    'english_q1': np.random.randint(60, 100, 200),\n",
    "    'english_q2': np.random.randint(60, 100, 200),\n",
    "    'english_q3': np.random.randint(60, 100, 200),\n",
    "    'english_q4': np.random.randint(60, 100, 200)\n",
    "})\n",
    "\n",
    "# Pivot table and reshaping exercises\n",
    "\n",
    "# 1. Melt wide format to long format\n",
    "students_long = # Your code here - melt score columns\n",
    "\n",
    "# 2. Extract subject and quarter from melted variable column\n",
    "students_long[['subject', 'quarter']] = # Your code here\n",
    "\n",
    "# 3. Create pivot table: students vs subjects with average scores\n",
    "student_subject_pivot = # Your code here\n",
    "\n",
    "# 4. Multi-level pivot table: grade_level and school vs subject\n",
    "grade_school_subject_pivot = # Your code here\n",
    "\n",
    "# 5. Pivot with multiple aggregation functions\n",
    "multi_agg_pivot = # Your code here - mean, min, max by subject and quarter\n",
    "\n",
    "# 6. Cross-tabulation with percentages\n",
    "# First, create performance categories\n",
    "def categorize_performance(score):\n",
    "    # Your code here - A, B, C, D categories\n",
    "    pass\n",
    "\n",
    "students_long['performance_category'] = # Your code here\n",
    "grade_performance_crosstab = # Your code here\n",
    "grade_performance_pct = # Your code here - normalize to percentages\n",
    "\n",
    "# 7. Stack and unstack operations\n",
    "stacked_data = # Your code here - stack the pivot table\n",
    "unstacked_data = # Your code here - unstack back\n",
    "\n",
    "# 8. Pivot with custom aggregation\n",
    "def score_range(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "score_range_pivot = # Your code here\n",
    "\n",
    "# 9. Create summary report format\n",
    "summary_report = # Your code here - comprehensive pivot table for reporting\n",
    "\n",
    "# 10. Handle missing values in pivot\n",
    "# Introduce some missing values\n",
    "students_with_missing = students_long.copy()\n",
    "students_with_missing.loc[students_with_missing.index % 50 == 0, 'value'] = np.nan\n",
    "pivot_with_missing = # Your code here - handle NaN appropriately\n",
    "\n",
    "# 11. Reshape back to original format\n",
    "back_to_wide = # Your code here - convert long back to wide\n",
    "\n",
    "# 12. Create percentage of total calculations\n",
    "pct_of_total_pivot = # Your code here - each cell as % of grand total\n",
    "\n",
    "print(\"Data Reshaping Results:\")\n",
    "print(f\"Original wide format: {students.shape}\")\n",
    "print(f\"Long format: {students_long.shape}\")\n",
    "print(f\"Student-subject pivot: {student_subject_pivot.shape}\")\n",
    "print(f\"Grade-school-subject pivot: {grade_school_subject_pivot.shape}\")\n",
    "print(\"\\nGrade vs Performance Cross-tab:\")\n",
    "print(grade_performance_crosstab)\n",
    "print(\"\\nSummary Report Sample:\")\n",
    "print(summary_report.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify reshaping operations\n",
    "print(f\"Melt increased rows: {len(students_long) > len(students)}\")\n",
    "print(f\"Subject and quarter extracted: {all(col in students_long.columns for col in ['subject', 'quarter'])}\")\n",
    "print(f\"Pivot table created: {student_subject_pivot.shape[0] == students['student_id'].nunique()}\")\n",
    "print(f\"Cross-tab percentages sum correctly: {np.allclose(grade_performance_pct.sum(axis=1), 100)}\")\n",
    "print(f\"Stack/unstack operations work: {unstacked_data.shape == stacked_data.unstack().shape}\")\n",
    "print(f\"Back to wide format: {back_to_wide.shape[1] >= students.shape[1] - 4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-9",
   "metadata": {},
   "source": [
    "### 9. Advanced Indexing and MultiIndex Operations (Mid-Advanced Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Complex data structures and hierarchical indexing for advanced analytics.\n",
    "\n",
    "**Key concepts**: MultiIndex, .set_index(), .reset_index(), level operations, .xs()\n",
    "\n",
    "**Common Questions**: \"Work with hierarchical data?\" \"Analyze data at different granularities?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hierarchical sales data\n",
    "np.random.seed(404)\n",
    "\n",
    "# Multi-level business data: Company -> Region -> Store -> Product -> Month\n",
    "companies = ['TechCorp', 'RetailGiant', 'FoodChain']\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "stores = [f'Store_{i}' for i in range(1, 6)]\n",
    "products = ['Product_A', 'Product_B', 'Product_C', 'Product_D']\n",
    "months = pd.date_range('2024-01-01', periods=12, freq='M')\n",
    "\n",
    "# Create hierarchical dataset\n",
    "hierarchical_data = []\n",
    "for company in companies:\n",
    "    for region in regions:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                for month in months:\n",
    "                    hierarchical_data.append({\n",
    "                        'company': company,\n",
    "                        'region': region,\n",
    "                        'store': store,\n",
    "                        'product': product,\n",
    "                        'month': month,\n",
    "                        'sales': np.random.randint(1000, 10000),\n",
    "                        'units': np.random.randint(50, 500),\n",
    "                        'cost': np.random.randint(500, 5000)\n",
    "                    })\n",
    "\n",
    "df_hierarchical = pd.DataFrame(hierarchical_data)\n",
    "\n",
    "# MultiIndex operations\n",
    "\n",
    "# 1. Create MultiIndex DataFrame\n",
    "multi_df = # Your code here - set multiple columns as index\n",
    "\n",
    "# 2. Access data at different levels\n",
    "techcorp_data = # Your code here - all TechCorp data\n",
    "north_region_data = # Your code here - all North region data\n",
    "specific_store = # Your code here - specific company-region-store combination\n",
    "\n",
    "# 3. Cross-section operations\n",
    "january_data = # Your code here - all January data across all levels\n",
    "product_a_data = # Your code here - all Product_A data\n",
    "\n",
    "# 4. Aggregations at different levels\n",
    "company_totals = # Your code here - aggregate by company\n",
    "region_totals = # Your code here - aggregate by company and region\n",
    "monthly_totals = # Your code here - aggregate by month across all levels\n",
    "\n",
    "# 5. Unstack operations for analysis\n",
    "company_product_matrix = # Your code here - companies vs products\n",
    "region_month_matrix = # Your code here - regions vs months\n",
    "\n",
    "# 6. Stack operations\n",
    "stacked_matrix = # Your code here - stack one of the matrices\n",
    "\n",
    "# 7. Swap index levels\n",
    "swapped_index = # Your code here - swap company and region levels\n",
    "\n",
    "# 8. Sort by index and values\n",
    "sorted_by_index = # Your code here - sort by index levels\n",
    "sorted_by_sales = # Your code here - sort by sales within each group\n",
    "\n",
    "# 9. Group operations with MultiIndex\n",
    "profit_by_level = # Your code here - calculate profit at different aggregation levels\n",
    "\n",
    "# 10. Reset and reconstruct index\n",
    "flattened = # Your code here - reset to flat structure\n",
    "reconstructed = # Your code here - recreate MultiIndex\n",
    "\n",
    "# 11. Advanced slicing\n",
    "advanced_slice = # Your code here - slice multiple levels simultaneously\n",
    "\n",
    "# 12. Index arithmetic and operations\n",
    "def calculate_market_share(group):\n",
    "    # Your code here - calculate market share within each group\n",
    "    pass\n",
    "\n",
    "market_share = # Your code here\n",
    "print(f\"MultiIndex Operations Results:\")\n",
    "print(f\"Original data shape: {df_hierarchical.shape}\")\n",
    "print(f\"MultiIndex data shape: {multi_df.shape}\")\n",
    "print(f\"Index levels: {multi_df.index.nlevels}\")\n",
    "print(f\"TechCorp data: {len(techcorp_data)}\")\n",
    "print(f\"Company totals shape: {company_totals.shape}\")\n",
    "print(f\"Company-product matrix shape: {company_product_matrix.shape}\")\n",
    "print(f\"Flattened data shape: {flattened.shape}\")\n",
    "print(f\"Reconstructed index levels: {reconstructed.index.nlevels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify MultiIndex operations\n",
    "print(f\"MultiIndex created: {isinstance(multi_df.index, pd.MultiIndex)}\")\n",
    "print(f\"Correct number of levels: {multi_df.index.nlevels == 5}\")\n",
    "print(f\"TechCorp filtering works: {len(techcorp_data) < len(multi_df)}\")\n",
    "print(f\"Aggregations reduce data size: {len(company_totals) < len(multi_df)}\")\n",
    "print(f\"Unstack creates matrix: {len(company_product_matrix.columns) > 1}\")\n",
    "print(f\"Reconstruction preserves structure: {reconstructed.index.nlevels == multi_df.index.nlevels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-10",
   "metadata": {},
   "source": [
    "### 10. Performance Optimization and Large Dataset Handling (Advanced Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hint-10",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Click for hint</summary>\n",
    "\n",
    "**Interview Focus**: Optimizing pandas operations for production environments and large datasets.\n",
    "\n",
    "**Key concepts**: Memory optimization, chunking, vectorization, categorical data, efficient I/O\n",
    "\n",
    "**Common Questions**: \"Handle datasets larger than memory?\" \"Optimize pandas performance?\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization techniques\n",
    "import time\n",
    "import gc\n",
    "np.random.seed(505)\n",
    "\n",
    "# Create large dataset for performance testing\n",
    "def create_large_dataset(n_rows=100000):\n",
    "    return pd.DataFrame({\n",
    "        'id': range(n_rows),\n",
    "        'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_rows),\n",
    "        'value': np.random.randn(n_rows),\n",
    "        'date': pd.date_range('2020-01-01', periods=n_rows, freq='H'),\n",
    "        'text': [f'text_{i%1000}' for i in range(n_rows)],\n",
    "        'flag': np.random.choice([True, False], n_rows)\n",
    "    })\n",
    "\n",
    "large_df = create_large_dataset()\n",
    "\n",
    "# Performance optimization exercises\n",
    "\n",
    "# 1. Memory optimization with data types\n",
    "def optimize_dtypes(df):\n",
    "    # Your code here - optimize data types for memory efficiency\n",
    "    pass\n",
    "\n",
    "memory_before = large_df.memory_usage(deep=True).sum()\n",
    "optimized_df = optimize_dtypes(large_df.copy())\n",
    "memory_after = optimized_df.memory_usage(deep=True).sum()\n",
    "\n",
    "# 2. Categorical data optimization\n",
    "def convert_to_categorical(df, columns):\n",
    "    # Your code here - convert specified columns to categorical\n",
    "    pass\n",
    "\n",
    "categorical_df = convert_to_categorical(large_df.copy(), ['category', 'text'])\n",
    "\n",
    "# 3. Chunked processing for large files\n",
    "def process_in_chunks(df, chunk_size=10000, operation=None):\n",
    "    # Your code here - process dataframe in chunks\n",
    "    pass\n",
    "\n",
    "chunked_result = process_in_chunks(large_df, operation=lambda x: x['value'].sum())\n",
    "\n",
    "# 4. Vectorized operations vs loops\n",
    "def compare_vectorized_vs_loop(df):\n",
    "    # Your code here - compare performance of vectorized vs loop operations\n",
    "    pass\n",
    "\n",
    "performance_comparison = compare_vectorized_vs_loop(large_df.head(10000))\n",
    "\n",
    "# 5. Efficient filtering techniques\n",
    "def efficient_filtering_comparison(df):\n",
    "    # Your code here - compare different filtering methods\n",
    "    pass\n",
    "\n",
    "filtering_results = efficient_filtering_comparison(large_df)\n",
    "\n",
    "# 6. Memory-efficient aggregations\n",
    "def memory_efficient_groupby(df):\n",
    "    # Your code here - use efficient groupby operations\n",
    "    pass\n",
    "\n",
    "efficient_agg = memory_efficient_groupby(large_df)\n",
    "\n",
    "# 7. Index optimization\n",
    "def optimize_index_operations(df):\n",
    "    # Your code here - demonstrate index optimization benefits\n",
    "    pass\n",
    "\n",
    "index_optimization = optimize_index_operations(large_df)\n",
    "\n",
    "# 8. Parallel processing simulation\n",
    "def simulate_parallel_processing(df, n_partitions=4):\n",
    "    # Your code here - simulate parallel processing by partitioning\n",
    "    pass\n",
    "\n",
    "parallel_result = simulate_parallel_processing(large_df)\n",
    "\n",
    "# 9. Memory monitoring\n",
    "def monitor_memory_usage(operation, *args, **kwargs):\n",
    "    # Your code here - monitor memory usage during operation\n",
    "    pass\n",
    "\n",
    "memory_stats = monitor_memory_usage(lambda df: df.groupby('category').sum(), large_df)\n",
    "\n",
    "# 10. Efficient I/O operations\n",
    "def efficient_io_demo():\n",
    "    # Your code here - demonstrate efficient reading/writing\n",
    "    pass\n",
    "\n",
    "io_results = efficient_io_demo()\n",
    "\n",
    "print(\"Performance Optimization Results:\")\n",
    "print(f\"Memory before optimization: {memory_before / 1024**2:.2f} MB\")\n",
    "print(f\"Memory after optimization: {memory_after / 1024**2:.2f} MB\")\n",
    "print(f\"Memory reduction: {(1 - memory_after/memory_before)*100:.1f}%\")\n",
    "print(f\"Categorical optimization: {categorical_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Chunked processing result: {chunked_result}\")\n",
    "print(f\"Performance comparison: {performance_comparison}\")\n",
    "print(f\"Filtering comparison: {filtering_results}\")\n",
    "print(f\"Index optimization: {index_optimization}\")\n",
    "print(f\"Memory monitoring: {memory_stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Verify performance optimizations\n",
    "print(f\"Memory optimization achieved: {memory_after < memory_before}\")\n",
    "print(f\"Categorical conversion worked: {categorical_df['category'].dtype.name == 'category'}\")\n",
    "print(f\"Chunked processing completed: {chunked_result is not None}\")\n",
    "print(f\"Performance comparison done: {performance_comparison is not None}\")\n",
    "print(f\"Large dataset handled: {len(large_df) == 100000}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3",
   "language": "python",
   "name": "python3.12.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
